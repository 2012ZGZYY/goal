\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\lstset{ %
  basicstyle=\scriptsize,
  numbers=left,
  frame=single,
}

\title{Forward automatic differentiation}
\author{Brian Granzow}
\begin{document}
\maketitle

\section{Introduction}

Automatic differentiation (AD) is a useful technique to computationally
evaluate analytic derivatives (to machine precision) of a given function.
AD leverages the fact that software programs are built as a sequence of
elementary operations and by applying the chain rule repeatedly to
these operations, derivatives of functions can be found. It is important to
note that AD is not the same thing symbolic differentiation or numerical
differentiation. There are two standard modes of automatic differentiation,
forward and reverse, as well as two methods of implementing the technique,
source code transformation and operator overloading. The present discussion
will be limited to the forward mode using operator overloading.

\section{Operator overloading}
Many modern programming languages provide the capability to overload
standard operators (e.g. $+, -, *,$ and $/$) so that different operators
have different implementations based on their arguments. In scientific
computing, operator overloading can be used to provide developers with the
ability to program using a notation much closer to the target mathematical
notation.

A common example of the use of operator overloading occurs when implementing
linear algebra features in a programming language. For instance, a developer
of a scientific code may implement implement a matrix class, called Matrix,
that stores double values in an $m$ x $n$ array. A standard approach to define
the addition of a matrix $A$ with another matrix $B$ would be to define a
function called `add', that takes as its two arguments two Matrix objects.
A user would then find the sum of the two matrices by calling the `add'
function
\begin{lstlisting}
Matrix A = ...
Matrix B = ...
Matrix C = add(A,B)
\end{lstlisting}

By overloading the addition operator $+$ to take two Matrix objects and
return a Matrix object with the appropriate sum, a user of the code can
program in a syntax much closer to the original mathematical syntax. The
sum of two Matrix objects could then be found by programming
\begin{lstlisting}
Matrix A = ...
Matrix B = ...
Matrix C = A + B
\end{lstlisting}

\section{Forward automatic differentiation}
In the forward mode of AD, a forward AD class is defined that contains both
a scalar value $x$ that corresponds to the value of the variable and a
derivative array $x'$ that corresponds to the values of derivatives of the
variable with respect to chosen independent variables. The computer program is
written in terms of these AD variables. At the beginning of the program,
appropriate AD variables, $x_i$ for $i = 1,2,3,...,n$, are initialized and
then set as the $i^{th}$ independent variable of a system of $n$ variables.
These AD variables (values and derivatives) are then propogated forward through
the code to compute some function $f(x_i)$.

Since the computation of $f$ is necessarily just a composition of elementary
operations, intermediate derivative arrays are updated via operator
overloading when a new operator is encountered. The operators are overloaded
with definitions from basic calculus rules. For example, the result for the
derivative array of an AD variable $a$ times another AD variable $b$, using
the $*$ operator, would be
\[
a*b = a'*b + a * b'
\]
where $a'*b$ is the scalar value of $b$ times $a$'s derivative array and
$a * b'$ is the scalar value of $a$ times $b$'s derivative array. Once
$f(x_i)$ is computed in the code, its value $f$ and its gradient vector
$\nabla f = f'$ are known. This is by virtue of the fact that derivatives
have been propogated forward through the code all the way to the evaluation
of $f$.

From an implementation point of view, automatic differentiation via
operator overloading is attractive in that existing codes can be easily
modified to obtain gradient information. For example, consider a code that
uses doubles for all of its evaluation types. A developer could simply
search and replace all of the instances of `double' in the code and replace
it with the appropriate AD type name. Through very little development
cost, gradient information is obtained.

\newpage

\section{A simple example}
To further illustrate the concept of forward AD, a simple example using
Sandia's Sacado AD library is examined in depth.
\begin{lstlisting}
#include <Sacado.hpp>

typedef Sacado::Fad::DFad<double> Fdouble;

Fdouble f(Fdouble x, Fdouble y)
{
  return x + y;
}

Fdouble g(Fdouble x, Fdouble y)
{
  return x * y;
}

int main()
{
  Fdouble x = 2.0;
  Fdouble y = 3.0;

  x.diff(0,2);
  y.diff(1,2);

  std::cout << x << std::endl;
  std::cout << y << std::endl;
  std::cout << f(x,y) << std::endl;
  std::cout << g(x,y) << std::endl;
  std::cout << f(x,y) * g(x,y) << std::endl;
}
\end{lstlisting}

\begin{itemize}
\item Line 1 includes Sacado, an automatic differentiation library
\item Line 3 defines the AD type to be called Fdouble
\item Line 5 declares a function $f$ of two AD variables
\item Line 7 defines the function $f(x,y) = x + y$
\item Line 10 declares a function $g$ of two AD variables
\item Line 12 defines the function $g(x,y) = x*y$
\item Line 17 initializes an AD variable $x$ to a value of $2$
\item Line 18 initializes an AD variable $y$ to a value of $3$
\item Line 20 sets the AD variable $x$ as the 1st independent variable in
a system of $2$ independent variables
\item Line 21 sets the AD variable $y$ as the 2nd independent variable in
a system of $2$ independent variables
\item Lines 23 - 27 print various resulting expressions from forward
automatic differentiation operations
\end{itemize}

The output from this simple example program is shown below

\begin{verbatim}
                    x:      2 [ 1 0 ]
                    y:      3 [ 0 1 ]
                    f:      5 [ 1 1 ]
                    g:      6 [ 3 2 ]
                    f*g:    30 [ 21 16 ]
\end{verbatim}

The first thing to note is that each AD variable has a scalar value
and a derivative array associated with it. The scalar value simply corresponds
to the value of the variable. For example, the value of $x=2$ and $y=3$
because they were initialized as such. The derivative array corresponds
to $[ \frac{\partial}{\partial x} \; \frac{\partial}{\partial y} ]$,
because it was set as such on lines 20 and 21 of the example program. It
should be obvious that the derivative array $x'$ of $x$ is simply $[1 \; 0]$
and that the derivative array $y'$ of $y$ is simply $[0 \; 1]$.

More interesting are the values and derivative arrays of $f, g$ and $f*g$.
Let's start with $f(x,y) = x + y$. The value of $f$ is clearly $2+3=5$. It
should also be clear that the value of $f$'s derivative array is $[1 \; 1]$.
To compute the derivative array for $f$, operator overloading is applied to
the $+$ operator for the Fdouble type and $f'$ is simply computed as the
sum of the derivative array $x'$ of $x$ and the derivative array $y'$ of $y$.
That is,
\begin{align*}
f' &= x' + y' \\
&= [1 \; 0] + [0 \; 1] \\
&= [1 \; 1]
\end{align*}

Taking a look at $g(x,y) = x*y$, it is clear that the value of $g = 2*3 = 6$.
The derivative array $g'$ of $g$ is computed by the product rule, again
via operator overloading. That is,
\begin{align*}
g' &= x' * y + x * y' \\
&= [1 \; 0] * 3 + 2 * [0 \; 1] \\
&= [3 \; 2]
\end{align*}

Finally, the derivative array $h'$ of $h = f(x,y) * g(x,y)$ is computed
by propogating the derivative arrays forward through the code.
\begin{align*}
(f * g)' &= f' * g + f * g' \\
&= [1 \; 1] * 6 + 5 * [3 \; 2] \\
&= [21 \; 16]
\end{align*}
where $f,f',g$ and $g'$ are computed in intermediate stages using the
methodology described above.

\end{document}
